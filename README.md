# AgentX: Dominance Protocol ‚ôüÔ∏è  
### Adaptive Reinforcement Learning Chess Intelligence

AgentX is an autonomous, reinforcement learning‚Äìbased chess agent designed to demonstrate **adaptive decision-making and strategy emergence** through interaction with its environment.  
Starting as a beginner, the agent improves continuously over multiple matches without manual reprogramming.

---

## üß† Problem Statement

Chess presents a **combinatorial complexity of approximately 10¬π¬≤‚Å∞ possible game states**, known as the **Shannon Number**.  
Traditional chess engines rely on brute-force search and handcrafted heuristics, which:

- Require massive computational resources  
- Are rigid and non-adaptive  
- Do not learn from experience  

Beginner players and lightweight systems struggle to improve in real time using such approaches.

### Our Solution

**AgentX** addresses this challenge using **Reinforcement Learning (RL)**.  
Instead of static evaluation functions, AgentX learns optimal strategies purely from interaction with the chess environment.

- No hardcoded strategies  
- No manual reprogramming  
- Intelligence emerges from experience  

---

## ‚öôÔ∏è Approach Overview

### Agent

- **AgentX** implements **Feature-Based Q-Learning**
- Learns an action-value function (Q-table)
- Uses an **epsilon-greedy policy** to balance exploration and exploitation
- Improves policy after every move and every match

### Environment

- Standard chess environment
- Board state represented using **FEN (Forsyth‚ÄìEdwards Notation)**
- Legal moves validated by chess engine logic
- Opponent is a heuristic-based ‚ÄúAggressive Random‚Äù player

### Observe‚ÄìAct‚ÄìLearn Flow

1. **Observe**  
   - Environment provides current board position as a FEN string  
   - Agent extracts high-level features from the position  

2. **Act**  
   - Agent selects a move using an epsilon-greedy policy  
   - Early matches favor exploration; later matches favor exploitation  

3. **Learn**  
   - Rewards and penalties are applied  
   - Q-values updated using the **Bellman Equation**  
   - Policy gradually converges toward optimal behavior  

This loop repeats every move and every match, driving continuous improvement.

---

## üßÆ Algorithm Details

### Core Algorithm
- **Q-Learning (Off-policy Temporal Difference Control)**

### Why Feature-Based Q-Learning?

The full chess state space (64 squares √ó 13 piece types) is computationally infeasible.  
AgentX compresses the state space using **feature engineering**, preserving strategic meaning while enabling efficient learning.

### State Space Compression

AgentX represents each position using **three high-impact features**:

1. **Material Balance**
   - Strong Lead (‚â• +9)
   - Slight Lead (+3 to +8)
   - Equal (¬±2)
   - Behind (‚â§ ‚àí3)

2. **Queen Status**
   - Active
   - Captured

3. **Game Phase**
   - Opening
   - Midgame
   - Endgame

This reduces complexity by orders of magnitude while retaining tactical relevance.

---

## üîç Exploration Strategy

AgentX uses **Linear Epsilon Decay** over a 20-match training window:

- Matches 1‚Äì5: 100% exploration  
- Matches 6‚Äì10: ~50% exploration  
- Matches 11‚Äì15: ~20% exploration  
- Match 16+: ~5% exploration  

This ensures:
- Early discovery of strategies  
- Late-stage policy stability and consistency  

---

## üìä Results & Expected Outputs

### Training Summary

- **Training Matches:** 20  
- **Policy Convergence:** By Match 8  
- **Peak Win Rate:** > 85% (final 5 matches)  
- **Final Average Reward:** ~ +15,000  
- **Q-Table Size:** ~1,200‚Äì1,500 unique state-action pairs  

### Emergent Behavior Observed

- Early games: Random, defensive play  
- Mid games: Tactical awareness and coordination  
- Late games: Aggressive dominance  

**Queen Hunter Pattern:**  
By Match 12, AgentX prioritizes attacking the opponent‚Äôs Queen, even sacrificing minor pieces, due to high reward (+3000).

**Technical Resignation Strategy:**  
When material advantage ‚â• +4, AgentX shifts to defensive consolidation to secure victory and avoid unnecessary risk.

---

## üõ†Ô∏è Setup Instructions

### Prerequisites

- Python 3.8+
- pip

### Installation

```bash
pip install python-chess numpy tkinter

